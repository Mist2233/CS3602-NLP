{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia-70M Baseline & KVPress 压缩实验\n",
    "\n",
    "本 Notebook 旨在评估无训练 KV Cache 压缩方法在 **Pythia-2.8B** 模型上的效果。\n",
    "\n",
    "**实验设置：**\n",
    "*   **模型**: `EleutherAI/pythia-2.8B`\n",
    "*   **方法**: Baseline (无压缩), RandomPress, KnormPress, ExpectedAttentionPress\n",
    "*   **数据集**: \n",
    "    *   `wikitext-2`: 标准 PPL 测试\n",
    "    *   `pg-19`: 超长文本测试 (取单一 sample)\n",
    "*   **指标**: Perplexity (PPL) 和 推理加速比 (Speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# 设置设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pythia的输入输出\n",
    "首先我们使用70m模型了解pythia的格式（即GPTNeoXAttention）\n",
    "\n",
    "- GPTNeoX 的forward函数显式接受 layer_past / cache_position / position_embeddings ，内部用 Cache 维护 KV。\n",
    "- Pythia 使用的是 DynamicCache + DynamicLayer 。\n",
    "- 每层 KV 的形状是 [batch, num_heads, seq_len, head_dim] 。\n",
    "- DynamicLayer.keys / values 是可读可写的张量属性（kvpress 里也就是这样直接赋值）。\n",
    "- get_seq_length() 等方法可以获取当前 cache 长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, hidden_states: torch.FloatTensor, attention_mask: torch.FloatTensor, head_mask: Optional[torch.FloatTensor] = None, layer_past: Optional[transformers.cache_utils.Cache] = None, output_attentions: Optional[bool] = False, cache_position: Optional[torch.LongTensor] = None, position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None, **kwargs: Unpack[transformers.modeling_flash_attention_utils.FlashAttentionKwargs])\n",
      "loading model...\n",
      "cache type: <class 'transformers.cache_utils.DynamicCache'>\n",
      "num layers: 6\n",
      "layer0 type: <class 'transformers.cache_utils.DynamicLayer'>\n",
      "keys shape: torch.Size([1, 8, 121, 64])\n",
      "values shape: torch.Size([1, 8, 121, 64])\n",
      "dir(layer0): ['batch_repeat_interleave', 'batch_select_indices', 'crop', 'device', 'dtype', 'get_mask_sizes', 'get_max_cache_shape', 'get_seq_length', 'is_compileable', 'is_initialized', 'is_sliding', 'keys', 'lazy_initialization', 'offload', 'prefetch', 'reorder_cache', 'reset', 'update', 'values']\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXAttention\n",
    "import inspect\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import Cache\n",
    "\n",
    "print(inspect.signature(GPTNeoXAttention.forward))\n",
    "\n",
    "\n",
    "model_id = \"EleutherAI/pythia-70m\"\n",
    "print(\"loading model...\")\n",
    "model_test = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "model_test.eval()\n",
    "\n",
    "text = \"Hello world, this is a streaming KV cache test. \" * 10\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "inputs = tok(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model_test(**inputs, use_cache=True)\n",
    "\n",
    "cache = out.past_key_values\n",
    "print(\"cache type:\", type(cache))\n",
    "print(\"num layers:\", len(cache.layers))\n",
    "layer0 = cache.layers[0]\n",
    "print(\"layer0 type:\", type(layer0))\n",
    "print(\"keys shape:\", layer0.keys.shape)\n",
    "print(\"values shape:\", layer0.values.shape)\n",
    "print(\"dir(layer0):\", [a for a in dir(layer0) if not a.startswith(\"_\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将StreamingLLM适配于pythia模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingLLM:\n",
    "    def __init__(self, n_sink: int = 4, window_size: int = 256):\n",
    "        self.n_sink = n_sink\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def build_context(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        if input_ids.size(1) <= self.n_sink + self.window_size:\n",
    "            return input_ids\n",
    "        sink = input_ids[:, : self.n_sink]\n",
    "        tail = input_ids[:, -self.window_size :]\n",
    "        return torch.cat([sink, tail], dim=1)\n",
    "\n",
    "    def compress_cache(self, cache) -> None:\n",
    "        for layer in cache.layers:\n",
    "            keys = layer.keys\n",
    "            values = layer.values\n",
    "            bsz, num_heads, seq_len, head_dim = keys.shape\n",
    "            if seq_len <= self.n_sink + self.window_size:\n",
    "                continue\n",
    "            device = keys.device\n",
    "            sink_end = min(self.n_sink, seq_len)\n",
    "            tail_len = min(self.window_size, seq_len - sink_end)\n",
    "            if tail_len <= 0:\n",
    "                keep_idx = torch.arange(0, sink_end, device=device)\n",
    "            else:\n",
    "                tail_start = seq_len - tail_len\n",
    "                keep_prefix = torch.arange(0, sink_end, device=device)\n",
    "                keep_tail = torch.arange(tail_start, seq_len, device=device)\n",
    "                keep_idx = torch.cat([keep_prefix, keep_tail], dim=0)\n",
    "            keys = keys.index_select(2, keep_idx)\n",
    "            values = values.index_select(2, keep_idx)\n",
    "            layer.keys = keys\n",
    "            layer.values = values\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        input_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 50,\n",
    "        temperature: float | None = None,\n",
    "        top_k: int | None = None,\n",
    "        top_p: float | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        device = next(model.parameters()).device\n",
    "        input_ids = input_ids.to(device)\n",
    "        outputs = model(input_ids, use_cache=True)\n",
    "        cache = outputs.past_key_values\n",
    "        self.compress_cache(cache)\n",
    "        generated = input_ids\n",
    "        last_token = generated[:, -1:]\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(last_token, use_cache=True, past_key_values=cache)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            cache = outputs.past_key_values\n",
    "            self.compress_cache(cache)\n",
    "            if temperature is not None and temperature > 0:\n",
    "                logits = logits / temperature\n",
    "            if top_k is not None and top_k > 0:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                min_values = v[:, -1].unsqueeze(-1)\n",
    "                logits = torch.where(logits < min_values, torch.full_like(logits, -float(\"inf\")), logits)\n",
    "            if top_p is not None and 0 < top_p < 1:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "                cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                logits = logits.masked_fill(indices_to_remove, -float(\"inf\"))\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            last_token = next_token\n",
    "        return generated\n",
    "\n",
    "def load_model_and_tokenizer(model_id: str, torch_dtype=torch.float16):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    return model, tokenizer\n",
    "\n",
    "def streaming_generate_from_text(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    n_sink: int = 4,\n",
    "    window_size: int = 256,\n",
    "    max_new_tokens: int = 50,\n",
    ") -> str:\n",
    "    wrapper = StreamingLLM(n_sink=n_sink, window_size=window_size)\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids\n",
    "    generated_ids = wrapper.generate(model, input_ids, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPL\n",
    "\n",
    "def evaluate_ppl_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    n_sink: int = 4,\n",
    "    window_size: int = 256,\n",
    "    max_tokens: int = 2000,\n",
    "):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids[:, :max_tokens].to(next(model.parameters()).device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    if seq_len < 2:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    stream = StreamingLLM(n_sink=n_sink, window_size=window_size)\n",
    "    cache = None\n",
    "    losses = []\n",
    "    ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for pos in range(seq_len - 1):\n",
    "        cur = input_ids[:, pos:pos + 1]\n",
    "        with torch.no_grad():\n",
    "            if cache is None:\n",
    "                outputs = model(cur, use_cache=True)\n",
    "            else:\n",
    "                outputs = model(cur, use_cache=True, past_key_values=cache)\n",
    "            cache = outputs.past_key_values\n",
    "            stream.compress_cache(cache)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            target = input_ids[:, pos + 1]\n",
    "            loss = ce(logits, target).mean()\n",
    "            losses.append(loss)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(losses).mean())\n",
    "    return ppl.item()\n",
    "\n",
    "def evaluate_ppl_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    max_tokens: int = 2000,\n",
    "):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids[:, :max_tokens].to(next(model.parameters()).device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    if seq_len < 2:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    max_length = model.config.max_position_embeddings\n",
    "    stride = 512\n",
    "    nlls = []\n",
    "    prev_end = 0\n",
    "    ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for begin in range(0, seq_len, stride):\n",
    "        end = min(begin + max_length, seq_len)\n",
    "        trg_len = end - prev_end\n",
    "        if trg_len <= 0:\n",
    "            break\n",
    "        cur = input_ids[:, begin:end]\n",
    "        target = cur.clone()\n",
    "        target[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(cur, labels=target)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        nlls.append(loss)\n",
    "        prev_end = end\n",
    "        if end == seq_len:\n",
    "            break\n",
    "\n",
    "    if not nlls:\n",
    "        return float(\"inf\")\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    return ppl.item()\n",
    "\n",
    "# Speed\n",
    "def benchmark_speed_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    n_sink: int = 4,\n",
    "    window_size: int = 256,\n",
    "    num_tokens: int = 50,\n",
    "    batch_size: int = 1,\n",
    "):\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(next(model.parameters()).device)\n",
    "    input_ids = input_ids.repeat(batch_size, 1)\n",
    "\n",
    "    stream = StreamingLLM(n_sink=n_sink, window_size=window_size)\n",
    "\n",
    "    start = time.time()\n",
    "    stream.generate(model, input_ids, max_new_tokens=num_tokens)\n",
    "    end = time.time()\n",
    "\n",
    "    duration = end - start\n",
    "    total_tokens = num_tokens * batch_size\n",
    "    tok_per_sec = total_tokens / duration\n",
    "    return tok_per_sec\n",
    "\n",
    "def benchmark_speed_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    num_tokens: int = 50,\n",
    "    batch_size: int = 1,\n",
    "):\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(next(model.parameters()).device)\n",
    "    input_ids = input_ids.repeat(batch_size, 1)\n",
    "\n",
    "    start = time.time()\n",
    "    _ = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=num_tokens,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    end = time.time()\n",
    "\n",
    "    duration = end - start\n",
    "    total_tokens = num_tokens * batch_size\n",
    "    tok_per_sec = total_tokens / duration\n",
    "    return tok_per_sec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估函数1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_speed_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    n_sink: int = 4,\n",
    "    window_size: int = 256,\n",
    "    num_tokens: int = 50,\n",
    "    batch_size: int = 1,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    input_ids = input_ids.repeat(batch_size, 1)\n",
    "\n",
    "    stream = StreamingLLM(n_sink=n_sink, window_size=window_size)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, use_cache=True)\n",
    "    cache = outputs.past_key_values\n",
    "    stream.compress_cache(cache)\n",
    "    generated = input_ids\n",
    "    last_token = generated[:, -1:]\n",
    "\n",
    "    ttft = None\n",
    "\n",
    "    for step in range(num_tokens):\n",
    "        step_start = time.time() if step == 0 else None\n",
    "        with torch.no_grad():\n",
    "            outputs = model(last_token, use_cache=True, past_key_values=cache)\n",
    "        cache = outputs.past_key_values\n",
    "        stream.compress_cache(cache)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        last_token = next_token\n",
    "        if step == 0:\n",
    "            ttft = time.time() - start\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "\n",
    "    token_phase_time = max(total_time - ttft, 1e-6)\n",
    "    total_tokens = num_tokens * batch_size\n",
    "    tpot = token_phase_time / total_tokens\n",
    "    throughput = total_tokens / token_phase_time\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    total_tokens_processed = (prompt_len + num_tokens) * batch_size\n",
    "    total_flops = 2.0 * num_params * total_tokens_processed\n",
    "    avg_flops_per_token = total_flops / total_tokens_processed\n",
    "\n",
    "    return {\n",
    "        \"ttft\": ttft,\n",
    "        \"tpot\": tpot,\n",
    "        \"throughput\": throughput,\n",
    "        \"total_time\": total_time,\n",
    "        \"total_flops\": total_flops,\n",
    "        \"avg_flops_per_token\": avg_flops_per_token,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_ppl_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    n_sink: int = 4,\n",
    "    window_size: int = 256,\n",
    "    max_tokens: int = 2000,\n",
    "):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids[:, :max_tokens].to(next(model.parameters()).device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    if seq_len < 2:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    stream = StreamingLLM(n_sink=n_sink, window_size=window_size)\n",
    "    cache = None\n",
    "    losses = []\n",
    "    ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for pos in range(seq_len - 1):\n",
    "        cur = input_ids[:, pos:pos + 1]\n",
    "        with torch.no_grad():\n",
    "            if cache is None:\n",
    "                outputs = model(cur, use_cache=True)\n",
    "            else:\n",
    "                outputs = model(cur, use_cache=True, past_key_values=cache)\n",
    "            cache = outputs.past_key_values\n",
    "            stream.compress_cache(cache)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            target = input_ids[:, pos + 1]\n",
    "            loss = ce(logits, target).mean()\n",
    "            losses.append(loss)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(losses).mean())\n",
    "    return ppl.item()\n",
    "\n",
    "\n",
    "def evaluate_ppl_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    max_tokens: int = 2000,\n",
    "):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids[:, :max_tokens].to(next(model.parameters()).device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    if seq_len < 2:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    max_length = model.config.max_position_embeddings\n",
    "    stride = 512\n",
    "    nlls = []\n",
    "    prev_end = 0\n",
    "    ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for begin in range(0, seq_len, stride):\n",
    "        end = min(begin + max_length, seq_len)\n",
    "        trg_len = end - prev_end\n",
    "        if trg_len <= 0:\n",
    "            break\n",
    "        cur = input_ids[:, begin:end]\n",
    "        target = cur.clone()\n",
    "        target[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(cur, labels=target)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        nlls.append(loss)\n",
    "        prev_end = end\n",
    "        if end == seq_len:\n",
    "            break\n",
    "\n",
    "    if not nlls:\n",
    "        return float(\"inf\")\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    return ppl.item()\n",
    "\n",
    "\n",
    "def benchmark_speed_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    num_tokens: int = 50,\n",
    "    batch_size: int = 1,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    input_ids = input_ids.repeat(batch_size, 1)\n",
    "\n",
    "    start = time.time()\n",
    "    ttft = None\n",
    "\n",
    "    generated = input_ids\n",
    "    cache = None\n",
    "\n",
    "    for step in range(num_tokens):\n",
    "        with torch.no_grad():\n",
    "            if cache is None:\n",
    "                outputs = model(generated, use_cache=True)\n",
    "            else:\n",
    "                outputs = model(generated[:, -1:], use_cache=True, past_key_values=cache)\n",
    "        cache = outputs.past_key_values\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        if step == 0:\n",
    "            ttft = time.time() - start\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "\n",
    "    token_phase_time = max(total_time - ttft, 1e-6)\n",
    "    total_tokens = num_tokens * batch_size\n",
    "    tpot = token_phase_time / total_tokens\n",
    "    throughput = total_tokens / token_phase_time\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    total_tokens_processed = (prompt_len + num_tokens) * batch_size\n",
    "    total_flops = 2.0 * num_params * total_tokens_processed\n",
    "    avg_flops_per_token = total_flops / total_tokens_processed\n",
    "\n",
    "    return {\n",
    "        \"ttft\": ttft,\n",
    "        \"tpot\": tpot,\n",
    "        \"throughput\": throughput,\n",
    "        \"total_time\": total_time,\n",
    "        \"total_flops\": total_flops,\n",
    "        \"avg_flops_per_token\": avg_flops_per_token,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/pythia-2.8b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki text length: 3092\n",
      "PG-19 text length: 100000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_long_text_from_dataset(\n",
    "    dataset_name: str = \"wikitext\",\n",
    "    split: str = \"test\",\n",
    "    limit_samples: int = 1,\n",
    "    max_chars: int | None = None,\n",
    ") -> str:\n",
    "    if dataset_name == \"wikitext\":\n",
    "        ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
    "        \n",
    "        # 正确获取文本的方法：\n",
    "        # 方法1：使用列表推导\n",
    "        texts = [item[\"text\"] for item in ds.select(range(limit_samples))]\n",
    "        # 或者方法2：直接切片\n",
    "        # texts = [ds[i][\"text\"] for i in range(min(limit_samples, len(ds)))]\n",
    "        \n",
    "        text = \"\\n\\n\".join(texts)\n",
    "    \n",
    "    elif dataset_name == \"pg19\":\n",
    "        ds = load_dataset(\"pg19\", split=split, streaming=True)\n",
    "        sample = next(iter(ds))\n",
    "        text = sample[\"text\"]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "    if max_chars is not None and len(text) > max_chars:\n",
    "        text = text[:max_chars]\n",
    "    \n",
    "    return text\n",
    "# 固定一次抽样\n",
    "text_wiki = load_long_text_from_dataset(\n",
    "    dataset_name=\"wikitext\",\n",
    "    split=\"train\",\n",
    "    limit_samples=100,\n",
    "    max_chars=100000,\n",
    ")\n",
    "\n",
    "text_pg19 = load_long_text_from_dataset(\n",
    "    dataset_name=\"pg19\",\n",
    "    split=\"test\",\n",
    "    limit_samples=1,\n",
    "    max_chars=100000,\n",
    ")\n",
    "\n",
    "print(\"Wiki text length:\", len(text_wiki))\n",
    "print(\"PG-19 text length:\", len(text_pg19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at C:\\Users\\35000\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\0.0.0\\b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Thu Dec 11 13:13:07 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集结构: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "训练集大小: 36718\n",
      "验证集大小: 3760\n",
      "测试集大小: 4358\n",
      "\n",
      "第一个样本:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 重新加载数据集\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "print(f\"数据集结构: {dataset}\")\n",
    "print(f\"训练集大小: {len(dataset['train'])}\")\n",
    "print(f\"验证集大小: {len(dataset['validation'])}\")\n",
    "print(f\"测试集大小: {len(dataset['test'])}\")\n",
    "\n",
    "# 查看第一个样本\n",
    "print(\"\\n第一个样本:\")\n",
    "print(dataset['train'][0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验主循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamingLLM sample:\n",
      " '在一座海边小城里，工程师正在测试一种新的 KV 缓存压缩算法。\\n我们从组件的字段描述中看，可以发现此设计中的锚点曾在一位工程师代码中改用过，这个锚点'\n",
      "Baseline PPL on custom_complex_dataset: 11.485358238220215\n",
      "StreamingLLM PPL(256) on custom_complex_dataset: 247.625\n",
      "StreamingLLM PPL(512) on custom_complex_dataset: 146.125\n",
      "Baseline speed (tok/s): 34.45938507354582\n",
      "StreamingLLM(256) KV-level speed (tok/s): 39.193901370098544\n",
      "StreamingLLM(512) KV-level speed (tok/s): 38.73939292733131\n",
      "\n",
      "=== Summary ===\n",
      "Baseline   - PPL: 11.485,  Speed: 34.46 tok/s\n",
      "Streaming(256)  - PPL: 247.625,  Speed: 39.19 tok/s\n",
      "Streaming(512)  - PPL: 146.125,  Speed: 38.74 tok/s\n"
     ]
    }
   ],
   "source": [
    "prompt = \"在一座海边小城里，工程师正在测试一种新的 KV 缓存压缩算法。\"\n",
    "\n",
    "# 生成效果看看（Streaming）\n",
    "encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = encoded.input_ids\n",
    "stream = StreamingLLM(n_sink=4, window_size=256)\n",
    "out_ids = stream.generate(model, input_ids, max_new_tokens=50)\n",
    "print(\"StreamingLLM sample:\\n\", repr(tokenizer.decode(out_ids[0], skip_special_tokens=False)))\n",
    "\n",
    "with open(\"text/long_normal.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 1) Baseline PPL\n",
    "ppl_base = evaluate_ppl_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "print(\"Baseline PPL on custom_complex_dataset:\", ppl_base)\n",
    "\n",
    "# 2) StreamingLLM PPL\n",
    "ppl_stream = evaluate_ppl_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    n_sink=4,\n",
    "    window_size=256,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "print(\"StreamingLLM PPL(256) on custom_complex_dataset:\", ppl_stream)\n",
    "\n",
    "# 2) StreamingLLM PPL\n",
    "ppl_stream1 = evaluate_ppl_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    n_sink=4,\n",
    "    window_size=512,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "print(\"StreamingLLM PPL(512) on custom_complex_dataset:\", ppl_stream1)\n",
    "\n",
    "# 3) Baseline Speed\n",
    "speed_base = benchmark_speed_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    num_tokens=50,\n",
    "    batch_size=1,\n",
    ")\n",
    "print(\"Baseline speed (tok/s):\", speed_base)\n",
    "\n",
    "# 4) StreamingLLM Speed\n",
    "speed_stream = benchmark_speed_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    n_sink=4,\n",
    "    window_size=256,\n",
    "    num_tokens=50,\n",
    "    batch_size=1,\n",
    ")\n",
    "print(\"StreamingLLM(256) KV-level speed (tok/s):\", speed_stream)\n",
    "\n",
    "speed_stream1 = benchmark_speed_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    n_sink=4,\n",
    "    window_size=512,\n",
    "    num_tokens=50,\n",
    "    batch_size=1,\n",
    ")\n",
    "print(\"StreamingLLM(512) KV-level speed (tok/s):\", speed_stream1)\n",
    "\n",
    "# 简单汇总\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Baseline   - PPL: {ppl_base:.3f},  Speed: {speed_base:.2f} tok/s\")\n",
    "print(f\"Streaming(256)  - PPL: {ppl_stream:.3f},  Speed: {speed_stream:.2f} tok/s\")\n",
    "print(f\"Streaming(512)  - PPL: {ppl_stream1:.3f},  Speed: {speed_stream1:.2f} tok/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
