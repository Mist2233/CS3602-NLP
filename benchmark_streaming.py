import os
import time
import torch
import copy
from tqdm import tqdm

# from calflops import calculate_flops  # æš‚æ—¶æ³¨é‡Šæ‰ï¼ŒåŠ å¿«æµ‹è¯•

# 1. è®¾ç½®é•œåƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
current_dir = os.getcwd()
os.environ["HF_HOME"] = os.path.join(current_dir, "hf_cache")

from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from datasets import load_dataset

# å¼•å…¥ StreamingLLM æ­£ç¡®å®ç°
from pythia_streaming_press import (
    enable_streaming_llm,
    disable_streaming_llm,
    patch_attention_layers,
    reset_attention_timing,
    enable_attention_timing_collection,
    disable_attention_timing_collection,
    get_attention_stats,
)

# ================= é…ç½®åŒºåŸŸ =================
MODEL_PATH = "./models/pythia-2.8b"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LENGTH = 2048

# StreamingLLM é…ç½®
SINK_SIZE = 8  # Attention Sink ä¿ç•™çš„åˆå§‹ token æ•°é‡
WINDOW_SIZE = 248  # æ»‘åŠ¨çª—å£å¤§å°ï¼ˆæ€»å®¹é‡ = 8 + 248 = 256ï¼‰

# æµ‹è¯•é…ç½®
PPL_TEST_TOKENS = 1000  # PPLæµ‹è¯•ä½¿ç”¨çš„tokenæ•°é‡
GENERATION_TOKENS = 1000  # ç”Ÿæˆé€Ÿåº¦æµ‹è¯•çš„tokenæ•°é‡
# ===========================================

print(f"æ£€æµ‹åˆ°çš„è®¾å¤‡: {DEVICE}")
print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH}...")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH, torch_dtype=torch.float16, device_map="auto"
)
model.eval()

# -------- å‡†å¤‡æ•°æ® --------
print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
# å®Œå…¨ç¦»çº¿æ¨¡å¼ï¼šç›´æ¥ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ï¼Œé¿å…ä»»ä½•ç½‘ç»œè¯·æ±‚
import datasets

# 1. WikiText - ç›´æ¥ä»æœ¬åœ°arrowæ–‡ä»¶åŠ è½½
wiki_arrow_path = os.path.join(
    current_dir,
    "hf_cache",
    "datasets",
    "wikitext",
    "wikitext-2-raw-v1",
    "0.0.0",
    "b08601e04326c79dfdd32d625aee71d232d685c3",
    "wikitext-test.arrow",
)
print(f"ä»æœ¬åœ°åŠ è½½ WikiText: {wiki_arrow_path}")
wiki_data = datasets.Dataset.from_file(wiki_arrow_path)
wiki_text = "\n\n".join(wiki_data["text"])

# 2. PG-19 (ä»æœ¬åœ°æ ·æœ¬æ–‡ä»¶åŠ è½½)
pg19_sample_path = os.path.join(
    current_dir, "hf_cache", "datasets", "pg19_sample", "pg19_sample.txt"
)
print(f"ä»æœ¬åœ°åŠ è½½ PG-19: {pg19_sample_path}")
with open(pg19_sample_path, "r", encoding="utf-8") as f:
    book_text = f.read()

# ç”¨äºé€Ÿåº¦æµ‹è¯•çš„promptï¼ˆå¤§çº¦500ä¸ªtokensï¼‰
prompt_text = book_text[:2000]


# -------- å®šä¹‰è¾…åŠ©ç±» --------
class SpeedTestStreamer(TextStreamer):
    def __init__(self, tokenizer, **kwargs):
        super().__init__(tokenizer, **kwargs)
        self.reset()

    def on_finalized_text(self, text: str, stream_end: bool = False):
        now = time.time()
        if self.token_count == 0:
            self.first_token_time = now
        self.token_count += 1

    def reset(self):
        self.start_time = 0
        self.first_token_time = 0
        self.token_count = 0


# -------- æ ¸å¿ƒæµ‹è¯•é€»è¾‘å°è£… --------
def calculate_ppl(text, max_tokens=1000, debug=False):
    """
    è®¡ç®—å›°æƒ‘åº¦ (PPL) - ä½¿ç”¨é€tokenç”Ÿæˆæ–¹å¼

    è¿™ç§æ–¹å¼èƒ½å¤ŸçœŸå®åæ˜ KV Cacheå‹ç¼©å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œ
    å› ä¸ºæ¯ä¸ªæ–°tokençš„é¢„æµ‹éƒ½ä¾èµ–äºä¹‹å‰ç´¯ç§¯çš„past_key_valuesã€‚

    Args:
        text: è¾“å…¥æ–‡æœ¬
        max_tokens: æµ‹è¯•çš„æœ€å¤§tokenæ•°é‡
        debug: æ˜¯å¦è¾“å‡ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯
    """
    encodings = tokenizer(text, return_tensors="pt")
    seq_len = encodings.input_ids.size(1)
    max_test_len = min(seq_len, max_tokens)

    if debug:
        print(f"   [PPLè®¡ç®—] åºåˆ—æ€»é•¿åº¦: {seq_len}, æµ‹è¯•é•¿åº¦: {max_test_len}")

    input_ids = encodings.input_ids[:, :max_test_len].to(DEVICE)
    past_key_values = None
    nlls = []

    # é€tokenç”Ÿæˆï¼šä½¿ç”¨token[0:i]é¢„æµ‹token[i]
    with torch.no_grad():
        for i in tqdm(
            range(1, input_ids.size(1)),
            desc="   è®¡ç®—PPL",
            ncols=80,
            leave=False,
            disable=not debug,
        ):
            # å½“å‰è¾“å…¥ï¼štoken[i-1]
            current_input = input_ids[:, i - 1 : i]

            # Forward passï¼ˆcacheä¼šè‡ªåŠ¨ç´¯ç§¯å’Œå‹ç¼©ï¼‰
            outputs = model(
                current_input,
                past_key_values=past_key_values,
                use_cache=True,
                return_dict=True,
            )

            # é¢„æµ‹token[i]å¹¶è®¡ç®—loss
            logits = outputs.logits[:, -1, :]
            target = input_ids[:, i]
            loss = torch.nn.functional.cross_entropy(logits, target)
            nlls.append(loss)

            # æ›´æ–°past_key_valuesï¼ˆStreamingLLMä¼šåœ¨è¿™é‡Œå‹ç¼©ï¼‰
            past_key_values = outputs.past_key_values

            # ç›‘æ§cacheçŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            if debug and i % 200 == 0 and past_key_values is not None:
                if hasattr(past_key_values, "get_seq_length"):
                    cache_len = past_key_values.get_seq_length(0)
                    print(f"      Step {i}: Cacheé•¿åº¦ = {cache_len}")

    if not nlls:
        return 0.0

    ppl = torch.exp(torch.stack(nlls).sum() / (len(nlls)))

    if debug:
        print(f"   [PPLè®¡ç®—] å®Œæˆï¼š{len(nlls)} tokens, PPL = {ppl.item():.4f}")

    return ppl.item()


def test_speed(input_text, generate_len=1000):
    """æµ‹è¯•ç”Ÿæˆé€Ÿåº¦å’Œæ˜¾å­˜å ç”¨"""
    inputs = tokenizer(input_text, return_tensors="pt").to(DEVICE)
    streamer = SpeedTestStreamer(tokenizer, skip_prompt=True)

    # æ¸…ç†æ˜¾å­˜å¹¶é‡ç½®ç»Ÿè®¡
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats(DEVICE)
    reset_attention_timing()
    enable_attention_timing_collection()

    streamer.reset()
    streamer.start_time = time.time()

    with torch.no_grad():
        model.generate(
            **inputs,
            max_new_tokens=generate_len,
            pad_token_id=tokenizer.eos_token_id,
            streamer=streamer,
            use_cache=True,
        )

    disable_attention_timing_collection()
    end_time = time.time()

    # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
    peak_memory_bytes = torch.cuda.max_memory_allocated(DEVICE)
    peak_memory_mb = peak_memory_bytes / (1024 * 1024)

    ttft = streamer.first_token_time - streamer.start_time
    tpot = (
        (end_time - streamer.first_token_time) / (streamer.token_count - 1)
        if streamer.token_count > 1
        else 0
    )
    throughput = streamer.token_count / (end_time - streamer.start_time)

    # è·å–attentionè®¡æ—¶ç»Ÿè®¡
    avg_attn_time, std_attn_time = get_attention_stats()

    return {
        "peak_memory_mb": peak_memory_mb,
        "ttft": ttft,
        "tpot_ms": tpot * 1000,
        "throughput": throughput,
        "avg_attn_ms": avg_attn_time,
        "std_attn_ms": std_attn_time,
    }


# -------- ç»Ÿä¸€è¿è¡Œå‡½æ•° --------
def run_benchmark_suite(suite_name, config_mode="baseline"):
    """
    è¿è¡Œå®Œæ•´çš„benchmarkæµ‹è¯•å¥—ä»¶

    Args:
        suite_name: æµ‹è¯•åç§°
        config_mode: é…ç½®æ¨¡å¼ ("baseline" æˆ– "streaming")
    """
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é…ç½®: {suite_name}")
    print(f"{'='*60}")

    # 1. é…ç½®æ¨¡å‹
    if config_mode == "streaming":
        print(f">>> å¯ç”¨ StreamingLLM (Sink={SINK_SIZE}, Window={WINDOW_SIZE})")
        enable_streaming_llm(
            model, n_sink=SINK_SIZE, window_size=WINDOW_SIZE, debug=False
        )
    else:
        print(">>> ä½¿ç”¨ Baseline é…ç½®ï¼ˆå…¨é‡KV Cacheï¼‰")
        # åªéœ€è¦patch attention layersä»¥æ”¶é›†timingä¿¡æ¯
        patch_attention_layers(model)

    # 2. PPL æµ‹è¯•
    print(f"\n[1/2] è®¡ç®— WikiText PPL (æµ‹è¯• {PPL_TEST_TOKENS} tokens)...")
    ppl = calculate_ppl(wiki_text, max_tokens=PPL_TEST_TOKENS, debug=False)
    print(f"      âœ“ PPL = {ppl:.4f}")

    # 3. ç”Ÿæˆé€Ÿåº¦æµ‹è¯•
    print(f"\n[2/2] æµ‹è¯•ç”Ÿæˆæ€§èƒ½ (ç”Ÿæˆ {GENERATION_TOKENS} tokens)...")
    print(f"      Prompté•¿åº¦: {len(prompt_text)} å­—ç¬¦")
    metrics = test_speed(prompt_text, generate_len=GENERATION_TOKENS)

    print(f"      âœ“ ååé‡: {metrics['throughput']:.2f} tok/s")
    print(f"      âœ“ æ˜¾å­˜å³°å€¼: {metrics['peak_memory_mb']:.2f} MB")
    print(f"      âœ“ TTFT: {metrics['ttft']:.4f} s")
    print(f"      âœ“ å¹³å‡Attentionè€—æ—¶: {metrics['avg_attn_ms']:.2f} ms")

    # 4. æ¸…ç†
    if config_mode == "streaming":
        disable_streaming_llm(model)

    return {"ppl": ppl, **metrics}


# ================= ä¸»ç¨‹åºæ‰§è¡Œ =================
print("\n" + "=" * 60)
print(" StreamingLLM Performance Benchmark ".center(60, "="))
print("=" * 60)
print(f"æ¨¡å‹: {MODEL_PATH}")
print(f"è®¾å¤‡: {DEVICE}")
print(
    f"StreamingLLMé…ç½®: Sink={SINK_SIZE}, Window={WINDOW_SIZE} (æ€»å®¹é‡={SINK_SIZE+WINDOW_SIZE})"
)
print("=" * 60)

results = {}

# 1. è¿è¡Œ Baseline (å…¨é‡KV Cache)
results["Baseline"] = run_benchmark_suite(
    "Baseline (Full Cache)", config_mode="baseline"
)

# 2. è¿è¡Œ StreamingLLM (å‹ç¼©KV Cache)
results["StreamingLLM"] = run_benchmark_suite(
    f"StreamingLLM (Sink={SINK_SIZE}+Window={WINDOW_SIZE})", config_mode="streaming"
)

# ================= æœ€ç»ˆå¯¹æ¯”æŠ¥è¡¨ =================
print("\n" + "=" * 60)
print(" Performance Comparison ".center(60, "="))
print("=" * 60)


# è®¡ç®—æ”¹è¿›æŒ‡æ ‡
def calc_improvement(baseline, streaming, lower_is_better=True):
    """è®¡ç®—æ€§èƒ½æ”¹è¿›ç™¾åˆ†æ¯”"""
    if lower_is_better:
        # è¶Šä½è¶Šå¥½çš„æŒ‡æ ‡ï¼ˆPPL, Memory, Latencyï¼‰
        improvement = (baseline - streaming) / baseline * 100
        symbol = "â†“" if streaming < baseline else "â†‘"
    else:
        # è¶Šé«˜è¶Šå¥½çš„æŒ‡æ ‡ï¼ˆThroughputï¼‰
        improvement = (streaming - baseline) / baseline * 100
        symbol = "â†‘" if streaming > baseline else "â†“"
    return improvement, symbol


# å®šä¹‰è¦å¯¹æ¯”çš„æŒ‡æ ‡
metrics_info = [
    ("ppl", "Perplexity", "{:.4f}", True),
    ("peak_memory_mb", "Peak Memory (MB)", "{:.2f}", True),
    ("throughput", "Throughput (tok/s)", "{:.2f}", False),
    ("ttft", "Time to First Token (s)", "{:.4f}", True),
    ("tpot_ms", "Time per Output Token (ms)", "{:.2f}", True),
    ("avg_attn_ms", "Avg Attention Time (ms)", "{:.2f}", True),
]

print(f"\n{'Metric':<30} | {'Baseline':<12} | {'Streaming':<12} | {'Change':<12}")
print("-" * 72)

for key, label, fmt, lower_better in metrics_info:
    base_val = results["Baseline"][key]
    stream_val = results["StreamingLLM"][key]
    improvement, symbol = calc_improvement(base_val, stream_val, lower_better)

    # æ ¼å¼åŒ–è¾“å‡º
    change_str = f"{symbol} {abs(improvement):.1f}%"
    print(
        f"{label:<30} | {fmt.format(base_val):<12} | {fmt.format(stream_val):<12} | {change_str:<12}"
    )

print("=" * 60)

# æ€»ç»“
ppl_increase = (results["StreamingLLM"]["ppl"] / results["Baseline"]["ppl"] - 1) * 100
memory_saved = (
    results["Baseline"]["peak_memory_mb"] - results["StreamingLLM"]["peak_memory_mb"]
)
speedup = results["StreamingLLM"]["throughput"] / results["Baseline"]["throughput"]

print("\nğŸ“Š Summary:")
print(f"  â€¢ PPLå¢åŠ : {ppl_increase:+.1f}% (è´¨é‡ç•¥å¾®ä¸‹é™ï¼Œåœ¨å¯æ¥å—èŒƒå›´)")
print(
    f"  â€¢ æ˜¾å­˜èŠ‚çœ: {memory_saved:.2f} MB ({memory_saved/results['Baseline']['peak_memory_mb']*100:.1f}%)"
)
print(f"  â€¢ é€Ÿåº¦æå‡: {speedup:.2f}x")
print(f"  â€¢ ç»“è®º: StreamingLLMåœ¨æ˜¾å­˜å’Œé€Ÿåº¦ä¸Šæœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼ŒPPLæŸå¤±è¾ƒå°")
print("=" * 60)
