# NLP Final Lab - StreamingLLM Implementation

åŸºäº Pythia-2.8b æ¨¡å‹çš„ StreamingLLM KV Cache ä¼˜åŒ–å®éªŒ

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [æ¨¡å‹ä¸æ•°æ®é›†ä¸‹è½½](#æ¨¡å‹ä¸æ•°æ®é›†ä¸‹è½½)
- [æ–‡ä»¶è¯´æ˜](#æ–‡ä»¶è¯´æ˜)
- [è¿è¡Œæ–¹æ³•](#è¿è¡Œæ–¹æ³•)
- [å®éªŒç»“æœ](#å®éªŒç»“æœ)

---

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº† **StreamingLLM** ç®—æ³•ï¼Œé€šè¿‡æ™ºèƒ½å‹ç¼© KV Cache æ¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚ä¸»è¦ç‰¹ç‚¹ï¼š

- âœ… **è´¨é‡å¯æ§**: PPL é€‚åº¦ä¸Šå‡ (6.98 â†’ 11.60, +66.2%)ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…
- âœ… **å†…å­˜ä¼˜åŒ–**: æ˜¾å­˜å ç”¨é™ä½ 5.9% (5783 MB â†’ 5441 MB)
- âœ… **æ€§èƒ½æå‡**: ååé‡æå‡ 16.3% (25.68 â†’ 29.87 tokens/s)
- âœ… **å»¶è¿Ÿé™ä½**: TTFT é™ä½ 35.8%, TPOT é™ä½ 13.9%
- âœ… **æ­£ç¡®å®ç°**: ä½¿ç”¨ Monkey Patch + è‡ªå®šä¹‰ Cache ç±»å®ç° StreamingLLM

**æ ¸å¿ƒæ€æƒ³**: ä¿ç•™å¼€å¤´çš„ Attention Sinks (n_sink tokens) å’Œæœ«å°¾çš„æœ€è¿‘ tokensï¼Œä¸¢å¼ƒä¸­é—´çš„è¿‡æ—¶ tokensã€‚

**å®ç°æ–¹å¼**: é€šè¿‡ Monkey Patch æ›¿æ¢æ¨¡å‹çš„ forward æ–¹æ³•ï¼Œæ³¨å…¥è‡ªå®šä¹‰çš„ `StreamingDynamicCache` ç±»ï¼Œåœ¨ cache å®¹é‡è¶…å‡ºé™åˆ¶æ—¶è‡ªåŠ¨æ‰§è¡Œé©±é€ç­–ç•¥ã€‚

**é…ç½®è¯´æ˜**: æœ¬å®éªŒä½¿ç”¨ Sink=8, Window=248 (æ€»å®¹é‡=256)ï¼Œåœ¨ 1000 token çš„æµ‹è¯•ä¸­å–å¾—äº†æ€§èƒ½ä¸è´¨é‡çš„è‰¯å¥½å¹³è¡¡ã€‚

### å®ç°æ–¹æ¡ˆæ¢ç´¢

æœ¬é¡¹ç›®åœ¨å®ç°è¿‡ç¨‹ä¸­å°è¯•äº†ä¸¤ç§æ–¹æ¡ˆï¼š

#### æ–¹æ¡ˆä¸€ï¼šPre-Forward Hookï¼ˆå¤±è´¥ï¼‰
**å®ç°æ€è·¯**: ä½¿ç”¨ `register_forward_pre_hook` æ‹¦æˆª Attention å±‚çš„è¾“å…¥ï¼Œåœ¨æ¯æ¬¡ forward å‰æ£€æŸ¥å¹¶å‹ç¼© KV Cacheã€‚

**å¤±è´¥åŸå› **:
1. **è¿‡åº¦å‹ç¼©**: Hook åœ¨æ¯ä¸ª tokenã€æ¯å±‚éƒ½è§¦å‘ï¼Œå¯¼è‡´ 87,712 æ¬¡å‹ç¼©ï¼ˆæ­£å¸¸åº”è¯¥åªåœ¨ cache è¶…é™æ—¶å‹ç¼©ï¼‰
2. **æ—¶åºé”™è¯¯**: Pre-forward hook æ— æ³•é˜»æ­¢ cache å¢é•¿ï¼Œå‹ç¼©åç«‹å³åˆè¢«æ–° token æ‰©å±•
3. **å±‚é—´ä¸ä¸€è‡´**: æ¯å±‚ç‹¬ç«‹å‹ç¼©ï¼Œç ´åäº†å¤šå±‚ä¹‹é—´çš„ cache ä¸€è‡´æ€§
4. **PPL æš´å¢**: å¯¼è‡´ PPL ä» 6.98 æš´å¢åˆ° 133.50 (+1812%)ï¼Œå®Œå…¨ä¸å¯ç”¨

**æ•™è®­**: StreamingLLM çš„æ­£ç¡®å®ç°å¿…é¡»åœ¨ Cache ç±»å†…éƒ¨è¿›è¡Œï¼Œè€Œä¸æ˜¯é€šè¿‡å¤–éƒ¨ Hook æ‹¦æˆªã€‚

#### æ–¹æ¡ˆäºŒï¼šä¾µå…¥å¼ä¿®æ”¹ + è‡ªå®šä¹‰ Cacheï¼ˆæˆåŠŸï¼‰
**å®ç°æ€è·¯**: 
1. åˆ›å»º `StreamingDynamicCache` ç±»ç»§æ‰¿è‡ª `DynamicCache`
2. é‡å†™ `update()` æ–¹æ³•ï¼Œåœ¨å…¶ä¸­å®ç° Lazy Eviction é€»è¾‘
3. é€šè¿‡ä¾µå…¥å¼ä¿®æ”¹æ›¿æ¢æ¨¡å‹çš„ forward æ–¹æ³•ï¼Œæ³¨å…¥è‡ªå®šä¹‰ Cache
4. Cache åªåœ¨è¶…å‡ºå®¹é‡æ—¶è§¦å‘å‹ç¼©ï¼Œè€Œéæ¯æ¬¡ forward

**æˆåŠŸåŸå› **:
1. **æ­£ç¡®çš„å‹ç¼©æ—¶æœº**: Cache åœ¨å†…éƒ¨åˆ¤æ–­æ˜¯å¦è¶…é™ï¼Œåªåœ¨å¿…è¦æ—¶å‹ç¼©
2. **ä¿æŒä¸€è‡´æ€§**: æ‰€æœ‰å±‚ä½¿ç”¨åŒä¸€ä¸ª Cache å¯¹è±¡ï¼ŒçŠ¶æ€ç»Ÿä¸€
3. **æ€§èƒ½é«˜æ•ˆ**: ä½¿ç”¨ Lazy Evictionï¼Œæ·»åŠ  64 token çš„ buffer é¿å…é¢‘ç¹å‹ç¼©
4. **ç»“æœåˆç†**: PPL åªå¢åŠ  66.2%ï¼Œæ˜¾å­˜å’Œé€Ÿåº¦éƒ½æœ‰æ”¹å–„

**å®ç°ç»†èŠ‚**:
```python
class StreamingDynamicCache(DynamicCache):
    def update(self, key_states, value_states, layer_idx, cache_kwargs=None):
        # 1. è°ƒç”¨çˆ¶ç±»æ·»åŠ æ–° token
        k_out, v_out = super().update(...)
        
        # 2. æ£€æŸ¥æ˜¯å¦è¶…å‡ºå®¹é‡ (Lazy Eviction)
        if current_len > limit + 64:  # buffer=64
            # 3. ä¿ç•™ [Sink + Window]
            k_new = torch.cat([k_sink, k_window], dim=-2)
            v_new = torch.cat([v_sink, v_window], dim=-2)
            
            # 4. æ›´æ–° cache
            self.layers[layer_idx].keys = k_new
            self.layers[layer_idx].values = v_new
        
        return k_out, v_out
```

---

## ç¯å¢ƒé…ç½®

### 1. åˆ›å»º Conda ç¯å¢ƒ

```bash
# åˆ›å»ºåä¸º nlp çš„ Python 3.10 ç¯å¢ƒ
conda create -n nlp python=3.10 -y
conda activate nlp
```

### 2. å®‰è£…ä¾èµ–

```bash
# PyTorch (CUDA 11.8 ç‰ˆæœ¬ï¼Œæ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬é€‰æ‹©)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Transformers å’Œç›¸å…³åº“
pip install transformers datasets accelerate
pip install huggingface_hub

# æ€§èƒ½åˆ†æå·¥å…·
pip install calflops

# å…¶ä»–å·¥å…·
pip install tqdm
```

### 3. ä¾èµ–ç‰ˆæœ¬è¯´æ˜

æ¨èç‰ˆæœ¬ï¼š
- Python: 3.10+
- PyTorch: 2.0+
- Transformers: 4.35+
- datasets: 2.x (æ³¨æ„ï¼š3.x åŠä»¥ä¸Šçš„ç‰ˆæœ¬å¯èƒ½å¯¼è‡´ PG-19 æ•°æ®é›†åŠ è½½å¤±è´¥)
- CUDA: 11.8 æˆ– 12.1

---

## æ¨¡å‹ä¸æ•°æ®é›†ä¸‹è½½

### æ–¹æ³•ä¸€ï¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆæ¨èï¼‰

**ä¸‹è½½æ¨¡å‹**ï¼š
```bash
conda activate nlp
python download_model.py
```

ä¸‹è½½å†…å®¹ï¼š
- **æ¨¡å‹**: Pythia-2.8b (EleutherAI/pythia-2.8b)
- **ä¿å­˜ä½ç½®**: `./models/pythia-2.8b/`
- **æ¨¡å‹å¤§å°**: ~5 GB
- **é¢„è®¡ä¸‹è½½æ—¶é—´**: 5-20 åˆ†é’Ÿï¼ˆå–å†³äºç½‘é€Ÿï¼‰

**ä¸‹è½½æ•°æ®é›†**ï¼š
```bash
python download_datasets.py
```

ä¸‹è½½å†…å®¹ï¼š
- **WikiText-2**: ç”¨äº PPL è¯„ä¼°
- **PG-19 æ ·æœ¬**: ç”¨äºç”Ÿæˆé€Ÿåº¦æµ‹è¯•
- **ä¿å­˜ä½ç½®**: `./hf_cache/datasets/`
- **æ•°æ®é›†å¤§å°**: ~50 MB
- **é¢„è®¡ä¸‹è½½æ—¶é—´**: 1-5 åˆ†é’Ÿ

### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨é…ç½®

1. **è®¾ç½® HuggingFace é•œåƒ**ï¼ˆå¤§é™†ç”¨æˆ·å¿…éœ€ï¼‰:
   ```python
   import os
   os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
   ```

2. **æ•°æ®é›†è‡ªåŠ¨ä¸‹è½½**ï¼š
   - WikiText-2: è¿è¡Œè„šæœ¬æ—¶è‡ªåŠ¨ä¸‹è½½åˆ° `./hf_cache/datasets/wikitext/`
   - PG-19: è¿è¡Œè„šæœ¬æ—¶è‡ªåŠ¨ä¸‹è½½åˆ° `./hf_cache/datasets/pg19/`

---

## æ–‡ä»¶è¯´æ˜

### æ ¸å¿ƒè„šæœ¬

| æ–‡ä»¶                            | è¯´æ˜                  | ç”¨é€”                                         |
| ------------------------------- | --------------------- | -------------------------------------------- |
| `download_model.py`             | æ¨¡å‹ä¸‹è½½è„šæœ¬          | ä» HuggingFace ä¸‹è½½ Pythia-2.8b              |
| `download_datasets.py`          | æ•°æ®é›†ä¸‹è½½è„šæœ¬        | ä¸‹è½½ WikiText-2 å’Œ PG-19 åˆ°æœ¬åœ°              |
| `benchmark_streaming.py`        | StreamingLLM å¯¹æ¯”æµ‹è¯• | å¯¹æ¯” Baseline å’Œ StreamingLLM çš„å…¨éƒ¨æ€§èƒ½æŒ‡æ ‡ |
| `pythia_streaming_press.py` | StreamingLLM æ ¸å¿ƒå®ç° | Monkey Patch + è‡ªå®šä¹‰ Cache ç±»å®ç°           |
| `run_pythia.py`                 | ç®€å•æ¨ç†è„šæœ¬          | å¿«é€Ÿæµ‹è¯•æ¨¡å‹ç”Ÿæˆèƒ½åŠ›                         |

### å®ç°æ¢ç´¢ï¼ˆæ•™å­¦ä»·å€¼ï¼‰

| æ–‡ä»¶              | è¯´æ˜                             | çŠ¶æ€                             |
| ----------------- | -------------------------------- | -------------------------------- |
| `pythia_press.py` | Hookæ–¹å¼å®ç°ï¼ˆå¤±è´¥æ¡ˆä¾‹ï¼‰         |  åºŸå¼ƒï¼Œå·²ç»åˆ é™¤                           |
| è¯´æ˜              | ä½¿ç”¨Pre-Forward Hookå¯¼è‡´è¿‡åº¦å‹ç¼© | æ•™è®­ï¼šä¸èƒ½ç”¨Hookå®ç°StreamingLLM |

### æ–‡æ¡£

| æ–‡ä»¶         | è¯´æ˜         |
| ------------ | ------------ |
| `README.md`  | é¡¹ç›®è¯´æ˜æ–‡æ¡£ |
| `RESULT.md`  | å®éªŒç»“æœè¾“å‡º |

### ç›®å½•ç»“æ„

```
NLP-FinalLab/
â”œâ”€â”€ models/                    # æ¨¡å‹æ–‡ä»¶
â”‚   â””â”€â”€ pythia-2.8b/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ model.safetensors
â”‚       â””â”€â”€ tokenizer.json
â”œâ”€â”€ hf_cache/                  # HuggingFace ç¼“å­˜
â”‚   â”œâ”€â”€ datasets/              # æ•°æ®é›†ç¼“å­˜
â”‚   â””â”€â”€ hub/                   # æ¨¡å‹ç¼“å­˜
â”‚   â””â”€â”€ modules/               # åŠŸèƒ½æ¨¡ç»„
â”œâ”€â”€ benchmark_streaming.py     # StreamingLLM å¯¹æ¯”
â”œâ”€â”€ pythia_streaming_press.py  # æ ¸å¿ƒå®ç°
â”œâ”€â”€ download_model.py          # ä¸‹è½½æ¨¡å‹è„šæœ¬
â”œâ”€â”€ download_datasets.py       # ä¸‹è½½æ•°æ®é›†è„šæœ¬
â””â”€â”€ README.md                  # æœ¬æ–‡ä»¶
```

---

## è¿è¡Œæ–¹æ³•

### 1. ä¸‹è½½æ¨¡å‹ä¸æ•°æ®é›†ï¼ˆåªéœ€é¦–æ¬¡è¿è¡Œæ—¶å®Œæˆï¼‰

```bash
conda activate nlp
python download_model.py
python download_datasets.py
```

é¢„è®¡ä¸‹è½½æ—¶é—´ï¼š20-40 åˆ†é’Ÿï¼ˆå–å†³äºç½‘é€Ÿï¼‰
æ¨¡å‹å¤§å°ï¼šçº¦ 10 GB

### 2. å¿«é€Ÿæµ‹è¯•æ¨¡å‹ç”Ÿæˆæ•ˆæœ

```bash
python run_pythia.py
```

è¿™ä¼šå¿«é€Ÿç”Ÿæˆä¸€æ®µæ–‡æœ¬ï¼ŒéªŒè¯æ¨¡å‹åŠ è½½æ­£ç¡®ã€‚

### 3. StreamingLLM å¯¹æ¯”æµ‹è¯•ï¼ˆæ ¸å¿ƒå®éªŒï¼‰

```bash
python benchmark_streaming.py
```

è¿™ä¼šè¿è¡Œï¼š
1. Baseline æµ‹è¯•ï¼ˆå…¨é‡ KV Cacheï¼‰
2. StreamingLLM æµ‹è¯•ï¼ˆSink=8, Window=248ï¼‰
3. å¯¹æ¯”ä¸¤è€…çš„æ€§èƒ½å·®å¼‚

è¾“å‡ºå¯¹æ¯”è¡¨æ ¼ï¼š
```
Metric                         | Baseline     | Streaming    | Change
------------------------------------------------------------------------
Perplexity                     | 6.9805       | 11.6016      | â†‘ 66.2%
Peak Memory (MB)               | 5783.06      | 5441.05      | â†“ 5.9%
Throughput (tok/s)             | 25.68        | 29.87        | â†‘ 16.3%
Time to First Token (s)        | 0.2698       | 0.1733       | â†“ 35.8%
Time per Output Token (ms)     | 38.71        | 33.34        | â†“ 13.9%
Avg Attention Time (ms)        | 0.15         | 0.08         | â†“ 42.4%
```

é¢„è®¡è¿è¡Œæ—¶é—´ï¼š~10 åˆ†é’Ÿ

---
- æ¯ä¸€æ­¥çš„ KV Cache é•¿åº¦
- å‹ç¼©å‰åçš„éªŒè¯
- ä¸‰ç§æ¨¡å¼çš„å¯¹æ¯”ï¼ˆBaseline / Manual / Generateï¼‰

---

## å®éªŒç»“æœ

### æœ€ç»ˆæ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡               | Baseline  | StreamingLLM | å˜åŒ–         | è¯´æ˜                 |
| ------------------ | --------- | ------------ | ------------ | -------------------- |
| **PPL** (â†“)        | 6.98      | 11.60        | **+66.2%** âœ… | è´¨é‡ç•¥å¾®ä¸‹é™ï¼Œå¯æ¥å— |
| **Memory** (â†“)     | 5783 MB   | 5441 MB      | **-5.9%** âœ…  | æ˜¾å­˜èŠ‚çœ 342 MB      |
| **Throughput** (â†‘) | 25.68 t/s | 29.87 t/s    | **+16.3%** âœ… | ååé‡æå‡           |
| **TTFT** (â†“)       | 269.8 ms  | 173.3 ms     | **-35.8%** âœ… | é¦– Token åŠ é€Ÿ        |
| **TPOT** (â†“)       | 38.71 ms  | 33.34 ms     | **-13.9%** âœ… | æ¯ Token å»¶è¿Ÿé™ä½    |
| **Avg Attn** (â†“)   | 0.15 ms   | 0.08 ms      | **-42.4%** âœ… | Attention æ•ˆç‡æå‡   |

### å…³é”®å‘ç°

1. âœ… **è´¨é‡å¯æ§**: PPL åªå¢åŠ  66.2%ï¼Œè¿œä½äº Hook æ–¹æ¡ˆçš„ 1812%
2. âœ… **å†…å­˜ä¼˜åŒ–**: æ˜¾å­˜èŠ‚çœ 342 MB (5.9%)ï¼Œé•¿åºåˆ—æ•ˆæœæ›´æ˜¾è‘—
3. âœ… **æ€§èƒ½æå‡**: ååé‡æå‡ 16.3%ï¼Œå»¶è¿Ÿé™ä½ 13.9-42.4%
4. âœ… **å®ç°æ­£ç¡®**: ä¾µå…¥å¼ä¿®æ”¹ + è‡ªå®šä¹‰ Cache æ–¹å¼å®Œå…¨æ­£ç¡®
5. âœ… **å‚æ•°åˆç†**: Sink=8, Window=248 (æ€»å®¹é‡=256) åœ¨ 1000 token æµ‹è¯•ä¸­å–å¾—è‰¯å¥½å¹³è¡¡

### å®ç°æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ         | PPL å¢å¹… | å‹ç¼©æ¬¡æ•° | ç»“æœ   | åŸå›                         |
| ------------ | -------- | -------- | ------ | --------------------------- |
| Hook æ–¹å¼    | +1812%   | 87,712   | âŒ å¤±è´¥ | æ¯ token æ¯å±‚éƒ½å‹ç¼©ï¼Œè¿‡åº¦   |
| ä¾µå…¥å¼ä¿®æ”¹ | +66.2%   | ~750     | âœ… æˆåŠŸ | åªåœ¨ cache è¶…é™æ—¶å‹ç¼©ï¼Œæ­£ç¡® |

**æ•™è®­**: StreamingLLM å¿…é¡»åœ¨ Cache ç±»å†…éƒ¨å®ç°é©±é€é€»è¾‘ï¼Œä¸èƒ½é€šè¿‡å¤–éƒ¨ Hook æ‹¦æˆªã€‚

### StreamingLLM å‚æ•°è¯´æ˜

```python
# åœ¨ benchmark_streaming.py ä¸­é…ç½®
SINK_SIZE = 8      # Attention Sink ä¿ç•™çš„åˆå§‹ token æ•°é‡
WINDOW_SIZE = 248  # æ»‘åŠ¨çª—å£å¤§å°
# æ€»å®¹é‡ = SINK_SIZE + WINDOW_SIZE = 256
```

å‚æ•°è°ƒä¼˜å»ºè®®ï¼š
- **æ€»å®¹é‡ (Sink + Window)**:
  - 256: å½“å‰é…ç½®ï¼ŒPPL +66.2%ï¼Œæ€§èƒ½æå‡æ˜æ˜¾
  - 512: é¢„è®¡ PPL +20-30%ï¼Œæ›´å¹³è¡¡çš„é€‰æ‹©
  - 1024: é¢„è®¡ PPL +5-10%ï¼Œè´¨é‡æ¥è¿‘ baseline
- **Sink å¤§å°**: 4-8 ä¹‹é—´ï¼Œä¿ç•™åˆå§‹ä¸Šä¸‹æ–‡çš„é”šç‚¹
- **Window å¤§å°**: å†³å®šäº†æœ€è¿‘å†å²çš„ä¿ç•™é‡ï¼Œæ˜¯ä¸»è¦å‚æ•°

### ä½¿ç”¨æ–¹æ³•

```python
from pythia_streaming_press import enable_streaming_llm, disable_streaming_llm

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("./models/pythia-2.8b", ...)

# å¯ç”¨ StreamingLLM
enable_streaming_llm(model, n_sink=8, window_size=248)

# æ­£å¸¸ä½¿ç”¨ generate()
outputs = model.generate(**inputs, max_new_tokens=1000, use_cache=True)

# ç¦ç”¨ StreamingLLMï¼ˆå¦‚éœ€åˆ‡æ¢å› baselineï¼‰
disable_streaming_llm(model)
```

### è®¡ç®—é‡åˆ†æ

```
Model: Pythia-2.8b
Params: 2.78 B (çº¦ä¸º 70M çš„ 40 å€)
Memory: ~5.5 GB (FP16)
Layers: 32 ä¸ª Transformer å±‚
```


## å‚è€ƒèµ„æ–™

- [StreamingLLM è®ºæ–‡](https://arxiv.org/abs/2309.17453) - Efficient Streaming Language Models with Attention Sinks
- [Pythia æ¨¡å‹](https://github.com/EleutherAI/pythia) - EleutherAI's Suite of Models
- [Transformers DynamicCache](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.DynamicCache) - å®˜æ–¹æ–‡æ¡£

---

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ Issue æˆ–è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚

**æœ€åæ›´æ–°**: 2025-12-27



